# DIForM - Local AI Setup Guide

## ğŸ¤– Two AI Systems Included

### Desktop/Backend: **Ollama** (Powerful)
- âœ… Llama 3, Mistral, CodeLlama, etc.
- âœ… Runs completely offline
- âœ… Full reasoning capabilities
- âœ… No API costs ever

### Web/Browser: **Transformers.js** (Lightweight)
- âœ… Runs in browser
- âœ… No backend needed
- âœ… Instant startup
- âœ… Perfect for web app

---

## ğŸš€ Quick Start - Ollama (Desktop/Backend)

### Step 1: Install Ollama

**macOS/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Download from [ollama.com/download](https://ollama.com/download)

### Step 2: Pull a Model

```bash
# Recommended: Llama 3.2 (3B) - Fast & capable
ollama pull llama3.2

# Or larger for better quality:
ollama pull llama3.2:latest  # 7B model
```

**Available Models:**
- `llama3.2` - Best balance (3B)
- `llama3.2:latest` - More capable (7B)
- `mistral` - Alternative (7B)
- `phi3` - Smallest (3.8B)

### Step 3: Start Ollama

```bash
ollama serve
```

Runs on `http://localhost:11434`

### Step 4: Configure DIForM

Edit `.env`:
```bash
OLLAMA_MODEL=llama3.2
OLLAMA_HOST=http://localhost:11434
```

### Step 5: Test It!

```bash
# Install new dependencies
npm install

# Start backend with AI
PORT=5001 npm run server
```

**Try a command:**
```
"Summarize my emails from last week and schedule follow-ups"
```

Now uses **real AI** to understand and plan!

---

## ğŸŒ Transformers.js (Web/Browser)

### Already Included!

The web app now uses Transformers.js automatically:

```bash
cd client
npm install
npm start
```

**First run:**
- Downloads ~100MB model (once)
- Caches in browser
- Works offline after download

**Features:**
- âœ… Text classification
- âœ… Sentiment analysis
- âœ… Command understanding
- âœ… No server needed

---

## ğŸ¯ How It Works

### Desktop App Flow

```
User Command
    â†“
Backend API
    â†“
Ollama (Local LLM)
    â†“
Real AI Analysis
    â†“
Action Plan
    â†“
Execute via Microsoft Graph
```

### Web App Flow

```
User Command
    â†“
Browser
    â†“
Transformers.js (In-browser AI)
    â†“
Quick Analysis
    â†“
Action Suggestions
    â†“
Backend API (optional)
```

---

## ğŸ’ª Model Comparison

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| phi3 | 3.8GB | âš¡âš¡âš¡ | â­â­ | Fast responses |
| llama3.2 (3B) | 3GB | âš¡âš¡âš¡ | â­â­â­ | **Recommended** |
| llama3.2 (7B) | 7GB | âš¡âš¡ | â­â­â­â­ | Better quality |
| mistral | 7GB | âš¡âš¡ | â­â­â­â­ | Alternative |
| llama3 (70B) | 40GB | âš¡ | â­â­â­â­â­ | Best (needs GPU) |

### System Requirements

**Minimum:**
- 8GB RAM
- 10GB disk space
- Modern CPU

**Recommended:**
- 16GB RAM
- 20GB disk space
- GPU (optional, 10x faster)

---

## ğŸ”§ Configuration Options

### Backend (.env)

```bash
# Ollama Configuration
OLLAMA_MODEL=llama3.2
OLLAMA_HOST=http://localhost:11434
OLLAMA_TIMEOUT=30000

# Fallback to keywords if Ollama unavailable
AI_FALLBACK=true
```

### Customize Prompts

Edit `server/services/aiService.js`:

```javascript
buildPrompt(command, context) {
  return `You are DIForM...
Custom instructions here...`;
}
```

---

## ğŸ§ª Testing

### Test Ollama Directly

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Summarize: Q3 sales up 25%"
}'
```

### Test via DIForM API

```bash
curl -X POST http://localhost:5001/api/process \
  -H "Content-Type: application/json" \
  -d '{
    "command": "Analyze my emails and schedule meetings"
  }'
```

---

## ğŸ¨ Advanced Usage

### GPU Acceleration

**NVIDIA GPU:**
```bash
# Ollama automatically uses GPU if available
nvidia-smi  # Check GPU
ollama run llama3.2  # Will use GPU
```

### Custom Models

```bash
# Fine-tune your own model
ollama create my-custom-model -f Modelfile
ollama run my-custom-model
```

### Multiple Models

```javascript
// Switch models per task
const models = {
  email: 'llama3.2',
  code: 'codellama',
  analysis: 'mistral'
};
```

---

## ğŸ“Š Performance Tips

### Faster Responses

1. **Use smaller models**: phi3 or llama3.2 (3B)
2. **Reduce context**: Shorter prompts
3. **Enable GPU**: 10x speed boost
4. **Increase RAM**: More for caching

### Better Quality

1. **Use larger models**: llama3.2 (7B) or mistral
2. **Better prompts**: More specific instructions
3. **Add examples**: Few-shot learning
4. **Fine-tune**: Train on your data

---

## ğŸ”’ Privacy Benefits

### 100% Local Processing
- âœ… No data sent to cloud
- âœ… No API keys needed
- âœ… No usage limits
- âœ… Works offline
- âœ… GDPR compliant
- âœ… Zero cost

### vs External APIs (OpenAI, etc.)
| Feature | Local (Ollama) | Cloud API |
|---------|---------------|-----------|
| Privacy | âœ… Complete | âŒ Data sent |
| Cost | âœ… Free | ğŸ’° Pay per use |
| Speed | âš¡ Instant | ğŸŒ Network delay |
| Offline | âœ… Works | âŒ Requires internet |
| Limits | âœ… None | âš ï¸ Rate limits |

---

## ğŸ› Troubleshooting

### Ollama won't start
```bash
# Check if running
curl http://localhost:11434

# Restart
killall ollama
ollama serve
```

### Model download fails
```bash
# Try different mirror
ollama pull llama3.2 --insecure
```

### Out of memory
```bash
# Use smaller model
ollama pull phi3

# Or reduce concurrent requests
```

### Slow responses
```bash
# Check GPU usage
nvidia-smi

# Use quantized models
ollama pull llama3.2:q4_0  # 4-bit quantized
```

---

## ğŸ¯ What You Get

### With Ollama Running:
- âœ… **Real AI understanding** of commands
- âœ… **Smart action planning** based on context
- âœ… **Natural language** processing
- âœ… **Context awareness** across tasks
- âœ… **Learning** from patterns

### With Transformers.js:
- âœ… **Instant** browser-based AI
- âœ… **No setup** required
- âœ… **Offline** after first load
- âœ… **Privacy** preserved
- âœ… **Fast** classification

---

## ğŸš€ Next Steps

1. âœ… Install Ollama
2. âœ… Pull llama3.2 model
3. âœ… Start Ollama server
4. âœ… Run DIForM backend
5. âœ… Test with real commands
6. âœ… Try different models
7. âœ… Fine-tune for your needs

---

**Both AI systems are now integrated and ready to use!** ğŸ‰

*Local AI. Zero API costs. Complete privacy.* ğŸ”’
